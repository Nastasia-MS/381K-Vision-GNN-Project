{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-geometric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8qh1n_-x5Jf",
        "outputId": "33e17402-33fb-4704-92d3-00e770632b13"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torch_geometric.nn import HypergraphConv, AttentionalAggregation\n"
      ],
      "metadata": {
        "id": "0LylL5fKw-Rr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(\n",
        "        mean=[0.4914, 0.4822, 0.4465],\n",
        "        std=[0.2470, 0.2435, 0.2616]\n",
        "    )\n",
        "    ])\n",
        "train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "print(train_dataset.data.shape)\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ],
      "metadata": {
        "id": "liJsPPJdmaSM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "972ac158-2d48-4623-a40d-2b0e3b884c5c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# image->hypergraph"
      ],
      "metadata": {
        "id": "9hz9GKBUmaqw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Xhd8DuGXlRN3"
      },
      "outputs": [],
      "source": [
        "def image_to_dynamic_hypergraph(images, k_spatial=4, k_feature=4):\n",
        "    batch_node_feats = []\n",
        "    batch_edge_index = []\n",
        "    batch_map = []\n",
        "    node_offset = 0\n",
        "\n",
        "    for b, img in enumerate(images):\n",
        "        # img: [C,H,W] -> patches\n",
        "        C, H, W = img.shape\n",
        "        patch_size = 8  # 8x8 patches for CIFAR-10\n",
        "        patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
        "        patches = patches.contiguous().view(C, -1, patch_size, patch_size)\n",
        "        patches = patches.view(patches.size(1), -1)  # flatten each patch to vector\n",
        "\n",
        "        num_nodes = patches.size(0)\n",
        "        node_feats = patches\n",
        "\n",
        "        # Spatial eges (static)\n",
        "        spatial_edges = []\n",
        "        coords = np.array([[i // (W // patch_size), i % (W // patch_size)] for i in range(num_nodes)])\n",
        "        for i in range(num_nodes):\n",
        "            dists = np.sum((coords - coords[i])**2, axis=1)\n",
        "            nn_idx = np.argsort(dists)[1:k_spatial+1]\n",
        "            for j in nn_idx:\n",
        "                spatial_edges.append([i, j])\n",
        "\n",
        "        # Feature hyperedges (dynamic, computed on GPU)\n",
        "        feats = node_feats\n",
        "        feats = feats.to(images.device)\n",
        "        dists_feat = torch.cdist(feats.float(), feats.float(), p=2)\n",
        "        feature_edges = []\n",
        "        for i in range(num_nodes):\n",
        "            nn_idx = torch.topk(dists_feat[i], k=k_feature+1, largest=False).indices[1:]\n",
        "            for j in nn_idx:\n",
        "                feature_edges.append([i, j])\n",
        "\n",
        "        all_edges = torch.tensor(spatial_edges + feature_edges, dtype=torch.long, device=images.device).T\n",
        "        batch_node_feats.append(node_feats)\n",
        "        batch_edge_index.append(all_edges + node_offset)\n",
        "        batch_map.append(torch.full((num_nodes,), b, dtype=torch.long, device=images.device))\n",
        "\n",
        "        node_offset += num_nodes\n",
        "\n",
        "    x = torch.cat(batch_node_feats, dim=0).float()\n",
        "    edge_index = torch.cat(batch_edge_index, dim=1)\n",
        "    batch_map = torch.cat(batch_map)\n",
        "    return x, edge_index, batch_map"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HyperVigClassifier(nn.Module):\n",
        "  def __init__(self, in_channels, hidden, num_classes):\n",
        "    super().__init__()\n",
        "    self.conv1 = HypergraphConv(in_channels, hidden)\n",
        "    self.conv2 = HypergraphConv(hidden, hidden)\n",
        "    self.conv3 = HypergraphConv(hidden, hidden)\n",
        "    self.pool = AttentionalAggregation(gate_nn=nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1)))\n",
        "    self.classifier = nn.Linear(hidden, num_classes)\n",
        "\n",
        "  def forward(self, x, edge_index, batch_map):\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    out = self.pool(x, batch_map) #attention pooling\n",
        "    out = self.classifier(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "MXckrB9QxtpA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = HyperVigClassifier(in_channels=3*8*8, hidden=256, num_classes=10).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(30):\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in train_loader:\n",
        "    images, labels = images.to(device), labels.to(device)\n",
        "    node_feats, edge_index, batch_map = image_to_dynamic_hypergraph(images)\n",
        "    node_feats, edge_index, batch_map = node_feats.to(device), edge_index.to(device), batch_map.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = model(node_feats, edge_index, batch_map)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    total_loss += loss.item() * images.size(0)\n",
        "    _, predicted = outputs.max(1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print(f\"Epoch {epoch+1}, Loss: {total_loss/total}, Accuracy: {correct/total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5U6ygnuvgsX",
        "outputId": "e1286283-3b3d-437b-e4f8-771befa20c78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.081924643859863, Accuracy: 0.23656\n",
            "Epoch 2, Loss: 1.9771157432556152, Accuracy: 0.275\n",
            "Epoch 3, Loss: 1.9334857147216797, Accuracy: 0.29718\n",
            "Epoch 4, Loss: 1.8997729174041749, Accuracy: 0.30732\n",
            "Epoch 5, Loss: 1.8751634169769287, Accuracy: 0.31582\n",
            "Epoch 6, Loss: 1.856643161430359, Accuracy: 0.32562\n"
          ]
        }
      ]
    }
  ]
}