# -*- coding: utf-8 -*-
"""Dynamic Hypergraph Edge Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvCUphCF-PAsJrY7090_HjtdZEPybaKT
"""

# pip install torch-geometric  # Commented out - for Colab use only, not needed in regular Python scripts

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as T
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
from torch_geometric.nn import HypergraphConv, AttentionalAggregation

transform = T.Compose([
    T.ToTensor(),
    T.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2470, 0.2435, 0.2616]
    )
    ])
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)

print(train_dataset.data.shape)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

"""# image->hypergraph"""

def image_to_dynamic_hypergraph_edge_attention(images, k_spatial=4, k_feature=4, edge_attn=None):
    batch_node_feats = []
    batch_edge_index = []
    batch_edge_weight = []
    batch_map = []
    node_offset = 0

    for b, img in enumerate(images):
        # img: [C,H,W] -> patches
        C, H, W = img.shape
        patch_size = 8
        patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
        patches = patches.permute(1,2,0,3,4).contiguous()       # [num_patches_h, num_patches_w, C, 8,8]
        patches = patches.view(-1, C, patch_size, patch_size)    # [num_nodes, C, 8,8]
        node_feats = patches.view(patches.size(0), -1).to(images.device)  # [num_nodes, 192]
        num_nodes = node_feats.size(0)

        # Spatial eges (static)
        spatial_edges = []
        coords = torch.tensor([[i // (W // patch_size), i % (W // patch_size)] for i in range(num_nodes)], device=images.device)
        dists = torch.cdist(coords.float(), coords.float(), p=2)
        for i in range(num_nodes):
            nn_idx = torch.topk(dists[i], k=k_spatial+1, largest=False).indices[1:]
            for j in nn_idx:
                spatial_edges.append([i, j])

        # Feature edges
        dists_feat = torch.cdist(node_feats.float(), node_feats.float(), p=2)
        feature_edges = []
        for i in range(num_nodes):
            nn_idx = torch.topk(dists_feat[i], k=k_feature+1, largest=False).indices[1:]
            for j in nn_idx:
                feature_edges.append([i, j])

        # Combine edges
        all_edges = torch.tensor(spatial_edges + feature_edges, dtype=torch.long, device=images.device).T

        # Compute edge weights if edge attention module is provided
        if edge_attn is not None:
            edge_weight = edge_attn(node_feats, all_edges)
        else:
            edge_weight = torch.ones(all_edges.size(1), device=images.device)

        batch_node_feats.append(node_feats)
        batch_edge_index.append(all_edges + node_offset)
        batch_edge_weight.append(edge_weight)
        batch_map.append(torch.full((num_nodes,), b, dtype=torch.long, device=images.device))
        node_offset += num_nodes

    x = torch.cat(batch_node_feats, dim=0).float()
    edge_index = torch.cat(batch_edge_index, dim=1)
    edge_weight = torch.cat(batch_edge_weight)
    batch_map = torch.cat(batch_map)
    return x, edge_index, edge_weight, batch_map

class EdgeAttention(nn.Module):
  def __init__(self, in_dim, hidden=64):
    super().__init__()
    self.mlp = nn.Sequential(
        nn.Linear(in_dim*2, hidden),
        nn.ReLU(),
        nn.Linear(hidden, 1)
    )

  def forward(self, x, edge_index):
    row, col = edge_index
    feats = torch.cat([x[row], x[col]], dim=1)
    alpha = self.mlp(feats).squeeze()
    return torch.sigmoid(alpha)

class HyperVigClassifier(nn.Module):
  def __init__(self, in_channels, hidden, num_classes):
    super().__init__()
    self.input_proj = nn.Linear(in_channels, hidden)
    self.conv1 = HypergraphConv(hidden, hidden)
    self.norm1 = nn.LayerNorm(hidden)
    self.dropout = nn.Dropout(0.3)
    self.conv2 = HypergraphConv(hidden, hidden)
    self.norm2 = nn.LayerNorm(hidden)
    self.dropout = nn.Dropout(0.3)
    self.conv3 = HypergraphConv(hidden, hidden)
    self.norm3 = nn.LayerNorm(hidden)
    self.dropout = nn.Dropout(0.3)
    #self.conv4 = HypergraphConv(hidden, hidden)
    #self.norm4 = nn.LayerNorm(hidden)
    self.ff = nn.Sequential(
    nn.Linear(hidden, hidden * 4),
    nn.ReLU(),
    nn.Linear(hidden * 4, hidden)
    )

    self.pool = AttentionalAggregation(gate_nn=nn.Sequential(nn.Linear(hidden, hidden), nn.ReLU(), nn.Linear(hidden, 1)))
    self.classifier = nn.Linear(hidden, num_classes)

  def forward(self, x, edge_index, edge_weight, batch_map):
    x = self.input_proj(x)  # match dimensions (192 -> 256)
    for conv, norm in [(self.conv1, self.norm1), (self.conv2, self.norm2), (self.conv3, self.norm3)]:
        x_res = x
        x = conv(x, edge_index, edge_weight)
        x = norm(x)
        x = self.dropout(F.relu(x)) + x_res
    x = self.ff(x)
    out = self.pool(x, batch_map)
    return self.classifier(out)

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = HyperVigClassifier(in_channels=3*8*8, hidden=256, num_classes=10).to(device)
edge_attn = EdgeAttention(in_dim=3*8*8, hidden=64).to(device)
optimizer = torch.optim.Adam(list(model.parameters()) + list(edge_attn.parameters()), lr=0.001)
criterion = nn.CrossEntropyLoss()

for epoch in range(100):
  model.train()
  total_loss = 0
  correct = 0
  total = 0
  for images, labels in train_loader:
    images, labels = images.to(device), labels.to(device)
    node_feats, edge_index, edge_weight, batch_map = image_to_dynamic_hypergraph_edge_attention(images, edge_attn=edge_attn)

    optimizer.zero_grad()
    outputs = model(node_feats, edge_index, edge_weight, batch_map)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    total_loss += loss.item() * images.size(0)
    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()

  print(f"Epoch {epoch+1}, Loss: {total_loss/total}, Accuracy: {correct/total}")