# -*- coding: utf-8 -*-
"""Dynamic Hypergraph Edge Attention.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HvCUphCF-PAsJrY7090_HjtdZEPybaKT
"""

# pip install torch-geometric  # Commented out - for Colab use only, not needed in regular Python scripts

import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as T
import torch.nn.functional as F
from torchvision.datasets import CIFAR10
from torch.utils.data import DataLoader
from torch_geometric.nn import HypergraphConv, AttentionalAggregation
import matplotlib
matplotlib.use('Agg')  # Use non-interactive backend for SSH
import matplotlib.pyplot as plt
import neptune
import random

# Training transforms with full AugReg stack (RandAugment, Random Erasing)
transform = T.Compose([
    T.RandAugment(num_ops=1, magnitude=5),
    T.ToTensor(),
    T.Normalize(
        mean=[0.4914, 0.4822, 0.4465],
        std=[0.2470, 0.2435, 0.2616]
    ),
    T.RandomErasing(p=0.25, scale=(0.02, 0.33), ratio=(0.3, 3.3), value=0)  # Random Erasing
])
train_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)
test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)

print(train_dataset.data.shape)
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

"""# image->hypergraph"""

class ConvStem(nn.Module):
    """Convolutional stem to extract features before graph construction""" #From GreedyViG
    def __init__(self):
        super().__init__()
        self.stem = nn.Sequential(
            nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm2d(64),
            nn.ReLU(),
        )
    
    def forward(self, x):
        return self.stem(x)

def image_to_dynamic_hypergraph_edge_attention(images, k_spatial=3, k_feature=3, edge_attn=None, patch_size=4):
    batch_node_feats = []
    batch_edge_index = []
    batch_edge_weight = []
    batch_map = []
    node_offset = 0

    for b, img in enumerate(images):
        # img: [C,H,W] -> patches
        C, H, W = img.shape
        patches = img.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)
        patches = patches.permute(1,2,0,3,4).contiguous()       # [num_patches_h, num_patches_w, C, patch_size, patch_size]
        patches = patches.view(-1, C, patch_size, patch_size)    # [num_nodes, C, patch_size, patch_size]
        node_feats = patches.view(patches.size(0), -1).to(images.device)  # [num_nodes, C*patch_size*patch_size]
        num_nodes = node_feats.size(0)

        # Spatial edges (static)
        spatial_edges = []
        # Create coordinates more efficiently and ensure correct device
        patch_w = W // patch_size
        coords = torch.zeros((num_nodes, 2), dtype=torch.float, device=images.device)
        coords[:, 0] = torch.arange(num_nodes, device=images.device) // patch_w
        coords[:, 1] = torch.arange(num_nodes, device=images.device) % patch_w
        dists = torch.cdist(coords.float(), coords.float(), p=2)
        for i in range(num_nodes):
            nn_idx = torch.topk(dists[i], k=k_spatial+1, largest=False).indices[1:]
            for j in nn_idx:
                spatial_edges.append([i, j])

        # Feature edges
        dists_feat = torch.cdist(node_feats.float(), node_feats.float(), p=2)
        feature_edges = []
        for i in range(num_nodes):
            nn_idx = torch.topk(dists_feat[i], k=k_feature+1, largest=False).indices[1:]
            for j in nn_idx:
                feature_edges.append([i, j])

        # Combine edges
        all_edges_list = spatial_edges + feature_edges
        if len(all_edges_list) == 0:
            # If no edges, create self-loops to prevent errors
            all_edges = torch.zeros((2, num_nodes), dtype=torch.long, device=images.device)
            all_edges[0] = torch.arange(num_nodes, device=images.device)
            all_edges[1] = torch.arange(num_nodes, device=images.device)
        else:
            all_edges = torch.tensor(all_edges_list, dtype=torch.long, device=images.device).T

        # Compute edge weights if edge attention module is provided
        if edge_attn is not None and all_edges.size(1) > 0:
            edge_weight = edge_attn(node_feats, all_edges)
        else:
            edge_weight = torch.ones(all_edges.size(1), device=images.device)

        batch_node_feats.append(node_feats)
        batch_edge_index.append(all_edges + node_offset)
        batch_edge_weight.append(edge_weight)
        batch_map.append(torch.full((num_nodes,), b, dtype=torch.long, device=images.device))
        node_offset += num_nodes

    # Handle empty batch case
    if len(batch_node_feats) == 0:
      raise ValueError("Empty batch: no images processed")
    
    x = torch.cat(batch_node_feats, dim=0).float()
    edge_index = torch.cat(batch_edge_index, dim=1) if len(batch_edge_index) > 0 else torch.zeros((2, 0), dtype=torch.long, device=x.device)
    edge_weight = torch.cat(batch_edge_weight) if len(batch_edge_weight) > 0 else torch.ones(0, device=x.device)
    batch_map = torch.cat(batch_map) if len(batch_map) > 0 else torch.zeros(0, dtype=torch.long, device=x.device)
    return x, edge_index, edge_weight, batch_map

class EdgeAttention(nn.Module):
  def __init__(self, in_dim, hidden=64):
    super().__init__()
    self.mlp = nn.Sequential(
        nn.Linear(in_dim*2, hidden),
        nn.ReLU(),
        nn.Dropout(0.1),
        nn.Linear(hidden, 1)
    )
    # Initialize weights properly
    self._init_weights()

  def _init_weights(self):
    for m in self.modules():
      if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight, gain=0.1)
        if m.bias is not None:
          nn.init.constant_(m.bias, 0)

  def forward(self, x, edge_index):
    row, col = edge_index
    # Handle empty edge_index case
    if edge_index.size(1) == 0:
      return torch.ones(0, device=x.device)
    feats = torch.cat([x[row], x[col]], dim=1)
    alpha = self.mlp(feats)
    # Handle both single and multi-dimensional cases
    if alpha.dim() > 1:
      alpha = alpha.squeeze(-1)
    else:
      alpha = alpha.squeeze()
    # Clamp to prevent extreme values
    alpha = torch.clamp(torch.sigmoid(alpha), min=0.01, max=0.99)
    return alpha

class HyperVigClassifier(nn.Module):
  def __init__(self, in_channels, hidden, num_classes):
    super().__init__()
    self.input_proj = nn.Linear(in_channels, hidden)
    self.conv1 = HypergraphConv(hidden, hidden)
    self.norm1 = nn.LayerNorm(hidden)
    self.conv2 = HypergraphConv(hidden, hidden)
    self.norm2 = nn.LayerNorm(hidden)
    self.conv3 = HypergraphConv(hidden, hidden)
    self.norm3 = nn.LayerNorm(hidden)
    self.dropout = nn.Dropout(0.1) # changed from 0.3 to 0.1 
    
    # More stable feedforward layer
    self.ff = nn.Sequential(
        nn.Linear(hidden, hidden * 2),
        nn.LayerNorm(hidden * 2),
        nn.ReLU(), 
        nn.Dropout(0.1), # changed from 0.2 to 0.1 
        nn.Linear(hidden * 2, hidden)
    )

    self.pool = AttentionalAggregation(gate_nn=nn.Sequential(
        nn.Linear(hidden, hidden), 
        nn.ReLU(),  
        nn.Dropout(0.1),
        nn.Linear(hidden, 1)
    ))
    # Added dropout to classifier 
    self.classifier = nn.Sequential(
        nn.Dropout(0.2),
        nn.Linear(hidden, num_classes)
    )
    
    # Initialize weights properly
    self._init_weights()

  def _init_weights(self):
    for m in self.modules():
      if isinstance(m, nn.Linear):
        nn.init.xavier_uniform_(m.weight, gain=0.1)
        if m.bias is not None:
          nn.init.constant_(m.bias, 0)

  def forward(self, x, edge_index, edge_weight, batch_map):
    x = self.input_proj(x)  # match dimensions to hidden
    
    for conv, norm in [(self.conv1, self.norm1), (self.conv2, self.norm2), (self.conv3, self.norm3)]:
        x_res = x
        x = conv(x, edge_index, edge_weight)
        x = norm(x)
        x = F.relu(x)  
        x = self.dropout(x) + x_res
    x = self.ff(x)
    out = self.pool(x, batch_map)
    out = self.classifier(out)
    return out

# Training hyperparameters - change max_epochs here and it will be used throughout
max_epochs = 100  # Number of epochs to train for 100 changed from 50 to 100 to increase training time

# Initialize Neptune monitoring
try:
    with open('neptune_api_key.txt', 'r') as f:
        api_token = f.read().strip()
    run = neptune.init_run(
        project="31K-ML-Real/381K-Vision-GNN-Project",
        api_token=api_token,
    )
    # Log hyperparameters
    run["parameters"] = {
        "learning_rate": 0.002, # changed from 0.001 to 0.002 to increase training time
        "weight_decay": 1e-5,
        "batch_size": 16,
        "hidden_dim": 256,
        "edge_attn_hidden": 64,
        "num_classes": 10,
        "patch_size": 4, # changed from 8 to 4 
        "stem_channels": 64,
        "in_channels": 1024,  # stem_channels * patch_size * patch_size (64 * 4 * 4)
        "k_spatial": 3, # changed from 4 to 3 
        "k_feature": 3, # changed from 4 to 3 
        "max_epochs": max_epochs,
        "scheduler_factor": 0.5,
        "scheduler_patience": 5,
        "device": 'cuda' if torch.cuda.is_available() else 'cpu',
        "mixup_alpha_start": 0.2,
        "mixup_alpha_end": 0.8,
        "cutmix_alpha_start": 0.2,
        "cutmix_alpha_end": 1.0,
        "augmentation_schedule": "linear_increase",
        "randaugment_ops": 1,
        "randaugment_magnitude": 5,
        "random_erasing_p": 0.25
    }
    neptune_enabled = True
    print("Neptune monitoring initialized successfully")
except Exception as e:
    print(f"Warning: Could not initialize Neptune monitoring: {e}")
    print("Continuing without Neptune monitoring...")
    neptune_enabled = False
    run = None

def mixup_data(x, y, alpha=0.8):
    """Mixup augmentation: sample-level blending"""
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

def mixup_criterion(criterion, pred, y_a, y_b, lam):
    """Mixup loss computation"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

def cutmix_data(x, y, alpha=1.0):
    """CutMix augmentation: patch-level blending"""
    lam = np.random.beta(alpha, alpha)
    batch_size = x.size()[0]
    index = torch.randperm(batch_size).to(x.device)
    
    # Get image dimensions (assuming [B, C, H, W])
    _, _, H, W = x.size()
    
    # Generate random bounding box
    cut_rat = np.sqrt(1.0 - lam)
    cut_w = int(W * cut_rat)
    cut_h = int(H * cut_rat)
    
    # Random center
    cx = np.random.randint(W)
    cy = np.random.randint(H)
    
    # Clamp bounding box
    bbx1 = np.clip(cx - cut_w // 2, 0, W)
    bby1 = np.clip(cy - cut_h // 2, 0, H)
    bbx2 = np.clip(cx + cut_w // 2, 0, W)
    bby2 = np.clip(cy + cut_h // 2, 0, H)
    
    # Apply CutMix
    x[:, :, bby1:bby2, bbx1:bbx2] = x[index, :, bby1:bby2, bbx1:bbx2]
    
    # Adjust lambda to match actual pixel ratio
    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (W * H))
    
    y_a, y_b = y, y[index]
    return x, y_a, y_b, lam

def cutmix_criterion(criterion, pred, y_a, y_b, lam):
    """CutMix loss computation"""
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)


device = 'cuda' if torch.cuda.is_available() else 'cpu'

# Initialize convolutional stem 
stem = ConvStem().to(device)

patch_size = 4  # Changed from 8 to 4 to prevent underfitting (more nodes per image)
stem_channels = 64  # ConvStem outputs 64 channels
in_channels = stem_channels * patch_size * patch_size  # 64 * 4 * 4 = 1024

hidden_dim = 256

model = HyperVigClassifier(in_channels=in_channels, hidden=hidden_dim, num_classes=10).to(device)
edge_attn = EdgeAttention(in_dim=in_channels, hidden=64).to(device)

# Lower learning rate and add weight decay for stability
# Include stem parameters in optimizer
optimizer = torch.optim.Adam(
    list(model.parameters()) + list(edge_attn.parameters()) + list(stem.parameters()), 
    lr=0.002, weight_decay=1e-5  # Increased learning rate (TA requirement)
)
# Add learning rate scheduler
scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)
# Criterion 
criterion = nn.CrossEntropyLoss()

# Augmentation schedule: start weak, increase over time
def get_augmentation_alpha(epoch, max_epochs, start_alpha=0.2, end_alpha_mixup=0.8, end_alpha_cutmix=1.0):
    """
    Gradually increase augmentation strength over training
    - Start with weak augmentation (alpha=0.2) for easier learning
    - Gradually increase to full strength by end of training
    """
    # Linear schedule: start_alpha -> end_alpha over max_epochs
    progress = epoch / max_epochs
    mixup_alpha = start_alpha + (end_alpha_mixup - start_alpha) * progress
    cutmix_alpha = start_alpha + (end_alpha_cutmix - start_alpha) * progress
    return mixup_alpha, cutmix_alpha

# Lists to store metrics for plotting
train_losses = []
train_accuracies = []
test_losses = []
test_accuracies = []

for epoch in range(max_epochs):
  model.train()
  edge_attn.train()  # Set edge attention to train mode
  stem.train()  # Set stem to train mode
  total_loss = 0
  correct = 0
  total = 0
  
  for images, labels in train_loader:
    images, labels = images.to(device), labels.to(device)
    
    optimizer.zero_grad()
    
    # Apply MixUp/CutMix with gradually increasing strength
    # Start weak (alpha=0.2) and increase to full strength over training
    mixup_alpha, cutmix_alpha = get_augmentation_alpha(epoch, max_epochs, 
                                                       start_alpha=0.2, 
                                                       end_alpha_mixup=0.8, 
                                                       end_alpha_cutmix=1.0)
    
    if random.random() < 0.5:
        # Apply MixUp with scheduled alpha
        images, labels_a, labels_b, lam = mixup_data(images, labels, alpha=mixup_alpha)
        use_cutmix = False
    else:
        # Apply CutMix with scheduled alpha
        images, labels_a, labels_b, lam = cutmix_data(images, labels, alpha=cutmix_alpha)
        use_cutmix = True

    # Apply convolutional stem before graph construction
    images = stem(images)
    
    # Convert augmented images to dynamic hypergraph format
    node_feats, edge_index, edge_weight, batch_map = image_to_dynamic_hypergraph_edge_attention(
        images, edge_attn=edge_attn, patch_size=patch_size
    )
    
    # Forward pass
    outputs = model(node_feats, edge_index, edge_weight, batch_map)
    
    # Apply appropriate loss (MixUp or CutMix)
    if use_cutmix:
        loss = cutmix_criterion(criterion, outputs, labels_a, labels_b, lam)
    else:
        loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)

    
    # Check for NaN before backward
    if torch.isnan(loss) or torch.isinf(loss):
      print(f"NaN/Inf detected at epoch {epoch+1}, batch, stopping training")
      break
    
    loss.backward()
    
    # Gradient clipping to prevent explosion (include stem)
    torch.nn.utils.clip_grad_norm_(
        list(model.parameters()) + list(edge_attn.parameters()) + list(stem.parameters()), 
        max_norm=1.0
    )
    
    optimizer.step()

    total_loss += loss.item() * images.size(0)
    _, predicted = outputs.max(1)
    total += labels.size(0)
    correct += (predicted == labels).sum().item()
  
  # Check for NaN in accumulated loss (after training loop)
  if total > 0:
    avg_loss = total_loss / total
    if np.isnan(avg_loss) or np.isinf(avg_loss):
      print(f"NaN/Inf detected in average loss at epoch {epoch+1}, stopping training")
      break
    
    train_loss = avg_loss
    train_acc = correct / total
    train_losses.append(train_loss)
    train_accuracies.append(train_acc)
    
    # Testing phase
    model.eval()
    edge_attn.eval()
    stem.eval()
    test_loss = 0
    test_correct = 0
    test_total = 0
    with torch.no_grad():
      for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)
        # Apply convolutional stem before graph construction
        images = stem(images)
        node_feats, edge_index, edge_weight, batch_map = image_to_dynamic_hypergraph_edge_attention(
            images, edge_attn=edge_attn, patch_size=patch_size
        )
        outputs = model(node_feats, edge_index, edge_weight, batch_map)
        loss = criterion(outputs, labels)
        
        test_loss += loss.item() * images.size(0)
        _, predicted = outputs.max(1)
        test_total += labels.size(0)
        test_correct += (predicted == labels).sum().item()
    
    # Handle division by zero
    if test_total > 0:
      test_loss = test_loss / test_total
      test_acc = test_correct / test_total
      test_losses.append(test_loss)
      test_accuracies.append(test_acc)
    else:
      test_loss = float('inf')
      test_acc = 0.0
      test_losses.append(test_loss)
      test_accuracies.append(test_acc)
      print(f"Warning: No test samples processed at epoch {epoch+1}")
    
    scheduler.step(avg_loss)
    current_lr = optimizer.param_groups[0]['lr']
    print(f"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.4f}, LR: {current_lr:.6f}")
    
    # Log metrics to Neptune
    if neptune_enabled and run is not None:
        run["train/loss"].append(train_loss)
        run["train/accuracy"].append(train_acc)
        run["test/loss"].append(test_loss)
        run["test/accuracy"].append(test_acc)
        run["learning_rate"].append(current_lr)
    
  else:
    print(f"Epoch {epoch+1}, No valid batches processed")
    break

# Plotting - only if we have data
if len(train_losses) > 0 and len(test_losses) > 0:
  try:
    plt.figure(figsize=(12, 5))

    # Plot loss
    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss', marker='o')
    plt.plot(range(1, len(test_losses) + 1), test_losses, label='Test Loss', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('Training and Test Loss')
    plt.legend()
    plt.grid(True)

    # Plot accuracy
    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy', marker='o')
    plt.plot(range(1, len(test_accuracies) + 1), test_accuracies, label='Test Accuracy', marker='s')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.title('Training and Test Accuracy')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.savefig('training_curves.png', dpi=150, bbox_inches='tight')
    print(f"\nTraining curves saved to 'training_curves.png'")
    
    # Upload plot to Neptune
    if neptune_enabled and run is not None:
        try:
            run["training_curves"].upload('training_curves.png')
            print("Training curves uploaded to Neptune")
        except Exception as e:
            print(f"Warning: Could not upload plot to Neptune: {e}")
    
    plt.close()  # Close the figure to free memory
  except Exception as e:
    print(f"Warning: Could not create plots: {e}")
else:
  print("Warning: No training data collected, skipping plot generation")

# Stop Neptune run
if neptune_enabled and run is not None:
    try:
        run.stop()
        print("Neptune run stopped successfully")
    except Exception as e:
        print(f"Warning: Could not stop Neptune run: {e}")